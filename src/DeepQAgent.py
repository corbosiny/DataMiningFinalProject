import argparse, retro, threading, os, numpy, random
from Agent import Agent
from LossHistory import LossHistory

from tensorflow.python import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
stepInput = False
class DeepQAgent(Agent):
    """ An agent that implements the Deep Q Neural Network Reinforcement Algorithm to learn.
    """

    stateIndicies = {512 : 0, 514 : 1, 516 : 2, 518 : 3, 520 : 4, 522 : 5, 524 : 6, 526 : 7, 532 : 8}  # Mapping between player state values and their one hot encoding index

    def getWeightsName():
        return  self.__class__.__name__ + "Weights"

    def getLogsName():
        return self.__class__.name + "logs"

    def __init__(self, state_size= 32, action_size= 12, game= 'StreetFighterIISpecialChampionEdition-Genesis', render= False, load= True, epsilon= 1):
        """Initializes the agent and the underlying neural network

        Parameters
        ----------
        state_size
            The number of features that will be fed into the Agent's network
        
        action_size
            The size of the possible buttons the Agent can press during a fight

        game
            A String of the game the Agent will be making an environment of, defaults to StreetFighterIISpecialChampionEdition-Genesis

        render
            A boolean that specifies whether or not to visually render the game while the Agent is playing

        Returns
        -------
        None
        """
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = 0.95                               # discount rate
        self.epsilon = epsilon                          # exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.lossHistory = LossHistory()

        if load:
            self.loadModel(os.path.join(Agent.DEFAULT_DIR_PATH, DeepQAgent.getWeightsName()))

        super(DeepQAgent, self).__init__(game= game, render= render) 

    def getMove(self, obs, info):
        """Returns a set of button inputs generated by the Agent's network after looking at the current observation

        Parameters
        ----------
        obs
            The observation of the current environment, 2D numpy array of pixel values

        info
            An array of information about the current environment, like player health, enemy health, matches won, and matches lost, etc.
            A full list of info can be found in data.json

        Returns
        -------
        move
            A set of button inputs in a multivariate array of the form Up, Down, Left, Right, A, B, X, Y, L, R.
        """
        if info['status'] not in DeepQAgent.stateIndicies.keys() or info['enemy_status'] not in DeepQAgent.stateIndicies.keys(): return [0] * self.action_size 
        if numpy.random.rand() <= self.epsilon:
            return self.getRandomMove()
        stateData = self.prepareNetworkInputs(info)
        move = self.model.predict(stateData)[0]
        move = [1 if value > 0 else 0 for value in move]
        return move

    def initializeNetwork(self):
        """Initializes a Neural Net for a Deep-Q learning Model
        
        Parameters   
        ----------
        None

        Returns
        -------
        None
        """
        self.model = Sequential()
        self.model.add(Dense(24, input_dim= self.state_size, activation='relu'))
        self.model.add(Dense(48, activation='relu'))
        self.model.add(Dense(96, activation='relu'))
        self.model.add(Dense(96, activation='relu'))
        self.model.add(Dense(48, activation='relu'))
        self.model.add(Dense(24, activation='relu'))
        self.model.add(Dense(self.action_size, activation='linear'))
        self.model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))

    def prepareNetworkInputs(self, step):
        """Generates a feature vector from thhe current game state information to feed into the network
        
        Parameters
        ----------
        step
            A given set of state information from the environment
            
        Returns
        -------
        feature vector
            An array extracted from the step that is the same size as the network input layer
            Takes the form of a 1 x 30 array. With the elements:
            enemy_health, enemy_x, enemy_y, 8 one hot encoded enemy state elements, 
            8 one hot encoded enemy character elements, player_health, player_x, player_y, and finally
            8 one hot encoded player state elements.
        """
        global stepInput
        feature_vector = []
        
        # Enemy Data
        feature_vector.append(step["enemy_health"])
        feature_vector.append(step["enemy_x_position"])
        feature_vector.append(step["enemy_y_position"])

        # one hot encode enemy state
        # enemy_status - 512 if standing, 514 if crouching, 516 if jumping, 518 blocking, 522 if normal attack, 524 if special attack, 526 if hit stun or dizzy, 532 if thrown
        oneHotEnemyState = [0] * len(DeepQAgent.stateIndicies.keys())
        if step['enemy_status'] not in [0, 528, 530, 1024, 1026, 1028, 1030, 1032]: oneHotEnemyState[DeepQAgent.stateIndicies[step["enemy_status"]]] = 1
        feature_vector += oneHotEnemyState

        # one hot encode enemy character
        oneHotEnemyChar = [0] * 8
        oneHotEnemyChar[step["enemy_character"]] = 1
        feature_vector += oneHotEnemyChar

        # Player Data
        feature_vector.append(step["health"])
        feature_vector.append(step["x_position"])
        feature_vector.append(step["y_position"])

        # player_status - 512 if standing, 514 if crouching, 516 if jumping, 520 blocking, 522 if normal attack, 524 if special attack, 526 if hit stun or dizzy, 532 if thrown
        oneHotPlayerState = [0] * len(DeepQAgent.stateIndicies.keys())
        if step['status'] not in [0, 528, 530, 1024, 1026, 1028, 1030, 1032]: oneHotPlayerState[DeepQAgent.stateIndicies[step["status"]]] = 1
        feature_vector += oneHotPlayerState

        feature_vector = numpy.reshape(feature_vector, [1, self.state_size])
        return feature_vector

    def trainNetwork(self):
        """To be implemented in child class, Runs through a training epoch reviewing the training data
        Parameters
        ----------
        None

        Returns
        -------
        None
        """
        minibatch = random.sample(self.memory, 5000)
        for state, action, reward, next_state, done in minibatch:
            target = reward
            if not done:
                target = (reward + self.gamma * numpy.amax(self.model.predict(next_state)[0]))
            target_f = self.model.predict(state)
            target_f[0][action] = target
            self.model.fit(state, target_f, epochs= 1, verbose= 0, callbacks= [history])
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        
        self.saveModel()

    def loadModel(self, name):
        """Loads in pretrained model weights
        Parameters
        ----------
        name
            String name of the file that the weights will be loaded from

        Returns
        -------
        None
        """
        self.model.load_weights(name)
        self.epsilon = self.epsilon_min

    def saveModel(self):
        """Saves the currently trained model weights
        Parameters
        ----------
        name
            String name of the file that the weights will be saved to

        Returns
        -------
        None
        """
        self.model.save_weights(os.path.join(Agent.DEFAULT_WEIGHTS_DIR_PATH, DeepQAgent.getWeightsName()))
        with open(os.path.join(Agent.DEFAULT_LOGS_DIR_PATH, DeepQAgent.getLogsName()), 'a+') as file:
            file.write(self.history)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description= 'Processes agent parameters.')
    parser.add_argument('-r', '--render', action= 'store_true', help= 'Boolean flag for if the user wants the game environment to render during play')
    parser.add_argument('-l', '--load', action= 'store_true', help= 'Boolean flag for if the user wants to load pre-existing weights')
    parser.add_argument('-e', '--episodes', type= int, default= 10, help= 'Intger representing the number of training rounds to go through, checkpoints are made at the end of each episode')
    args = parser.parse_args()
    qAgent = DeepQAgent(render= args.render, load= args.load)
    qAgent.load(os.path.join(Agent.DEFAULT_DIR_PATH, DeepQAgent.getWeightsName()))
    qAgent.train(review= True, episodes= args.episodes)
