import argparse, retro, threading, os, numpy, random, math
from Agent import Agent
from LossHistory import LossHistory

from tensorflow.python import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from keras.models import load_model
from collections import deque

class DeepQAgent(Agent):
    """An agent that implements the Deep Q Neural Network Reinforcement Algorithm to learn street fighter 2"""
    
    doneKeys = [1024, 1026, 1028, 1032]

    # Mapping between player state values and their one hot encoding index
    stateIndices = {512 : 0, 514 : 1, 516 : 2, 518 : 3, 520 : 4, 522 : 5, 524 : 6, 526 : 7, 532 : 8} 

    def isActionableState(state):
        """Determines if the Agent has control over the game in it's current state(the Agent is in hit stun, ending lag, etc.)

        Parameters
        ----------
        state
            The RAM info of the current game state the Agent is presented with as a dictionary of keyworded values from Data.json

        Returns
        -------
        isActionable
            A boolean variable describing whether the Agent has control over the given state of the game
        """
        if state['status'] not in list(DeepQAgent.stateIndices.keys()) or state['enemy_status'] not in list(DeepQAgent.stateIndices.keys()):
            return False
        else:
            return True

    def sigmoid(x):
        """Applies the sigmoid function to x

        Parameters
        ----------
        x
            Any real number
        Returns
        -------
        activation score
            The resultant value of the sigmoid function applied to x
        """
        return 1 / (1 + math.exp(-x))

    def __init__(self, state_size= 32, action_size= 12, game= 'StreetFighterIISpecialChampionEdition-Genesis', render= False, load= False, epsilon= .05):
        """Initializes the agent and the underlying neural network

        Parameters
        ----------
        state_size
            The number of features that will be fed into the Agent's network
        
        action_size
            The size of the possible buttons the Agent can press during a fight

        game
            A String of the game the Agent will be making an environment of, defaults to StreetFighterIISpecialChampionEdition-Genesis

        render
            A boolean that specifies whether or not to visually render the game while the Agent is playing

        Returns
        -------
        None
        """
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = 0.95                               # discount rate
        self.epsilon = epsilon                          # exploration rate
        self.epsilon_decay = 0.995
        self.learning_rate = 0.001
        self.lossHistory = LossHistory()
        super(DeepQAgent, self).__init__(game= game, render= render, load= load) 

    def getMove(self, obs, info):
        """Returns a set of button inputs generated by the Agent's network after looking at the current observation

        Parameters
        ----------
        obs
            The observation of the current environment, 2D numpy array of pixel values

        info
            An array of information about the current environment, like player health, enemy health, matches won, and matches lost, etc.
            A full list of info can be found in data.json

        Returns
        -------
        move
            A set of button inputs in a multivariate array of the form Up, Down, Left, Right, A, B, X, Y, L, R.
        """
        if not DeepQAgent.isActionableState(info):
            return [0] * self.action_size
        elif numpy.random.rand() <= self.epsilon:
            return self.getRandomMove()
        else:
            stateData = self.prepareNetworkInputs(info)
            move = self.model.predict(stateData)[0]
            move = [1 if value > .5 else 0 for value in move]
            return move

    def initializeNetwork(self):
        """Initializes a Neural Net for a Deep-Q learning Model
        
        Parameters   
        ----------
        None

        Returns
        -------
        model
            The initialized neural network model that Agent will interface with to generate game moves
        """
        model = Sequential()
        model.add(Dense(24, input_dim= self.state_size, activation='relu'))
        model.add(Dense(48, activation='relu'))
        model.add(Dense(96, activation='relu'))
        model.add(Dense(48, activation='relu'))
        model.add(Dense(24, activation='relu'))
        model.add(Dense(self.action_size, activation='sigmoid'))
        model.compile(loss='binary_crossentropy', optimizer=Adam(lr=self.learning_rate))
        print('Successfully initialized model')
        return model

    def prepareMemoryForTraining(self, memory):
        """prepares the recorded fight sequences into training data
        
        Parameters
        ----------
        memory
            A 2D array where each index is a recording of a state, action, new state, and reward sequence
            See readme for more details

        Returns
        -------
        data
            The prepared training data in whatever from the model needs to train
            DeepQ needs a state, action, and reward sequence to train on
            The observation data is thrown out for this model for training
        """
        data = []
        for index, step in enumerate(self.memory):
            state = step[Agent.STATE_INDEX]
            if DeepQAgent.isActionableState(state):
                data.append(
                [self.prepareNetworkInputs(step[Agent.STATE_INDEX]), 
                step[Agent.ACTION_INDEX], 
                step[Agent.REWARD_INDEX]])

        return data

    def prepareNetworkInputs(self, step):
        """Generates a feature vector from the current game state information to feed into the network
        
        Parameters
        ----------
        step
            A given set of state information from the environment
            
        Returns
        -------
        feature vector
            An array extracted from the step that is the same size as the network input layer
            Takes the form of a 1 x 30 array. With the elements:
            enemy_health, enemy_x, enemy_y, 8 one hot encoded enemy state elements, 
            8 one hot encoded enemy character elements, player_health, player_x, player_y, and finally
            8 one hot encoded player state elements.
        """
        feature_vector = []
        
        # Enemy Data
        feature_vector.append(step["enemy_health"])
        feature_vector.append(step["enemy_x_position"])
        feature_vector.append(step["enemy_y_position"])

        # one hot encode enemy state
        # enemy_status - 512 if standing, 514 if crouching, 516 if jumping, 518 blocking, 522 if normal attack, 524 if special attack, 526 if hit stun or dizzy, 532 if thrown
        oneHotEnemyState = [0] * len(DeepQAgent.stateIndices.keys())
        oneHotEnemyState[DeepQAgent.stateIndices[step["enemy_status"]]] = 1
        feature_vector += oneHotEnemyState

        # one hot encode enemy character
        oneHotEnemyChar = [0] * 8
        oneHotEnemyChar[step["enemy_character"]] = 1
        feature_vector += oneHotEnemyChar

        # Player Data
        feature_vector.append(step["health"])
        feature_vector.append(step["x_position"])
        feature_vector.append(step["y_position"])

        # player_status - 512 if standing, 514 if crouching, 516 if jumping, 520 blocking, 522 if normal attack, 524 if special attack, 526 if hit stun or dizzy, 532 if thrown
        oneHotPlayerState = [0] * len(DeepQAgent.stateIndices.keys())
        oneHotPlayerState[DeepQAgent.stateIndices[step["status"]]] = 1
        feature_vector += oneHotPlayerState

        feature_vector = numpy.reshape(feature_vector, [1, self.state_size])
        return feature_vector

    def trainNetwork(self, data, model):
        """To be implemented in child class, Runs through a training epoch reviewing the training data
        Parameters
        ----------
        data
            The training data for the model to train on, a 2D array of state, action, reward, sequence

        model
            The model to train and return the Agent to continue playing with
        Returns
        -------
        model
            The input model now updated after this round of training on data
        """
        minibatch = random.sample(data, len(data))
        for state, action, reward in minibatch:       
            modelOutput = model.predict(state)[0]

            reward = DeepQAgent.sigmoid(reward)
            for index, buttonPress in enumerate(action):
                if buttonPress: modelOutput[index] = reward

            modelOutput = numpy.reshape(modelOutput, [1, self.action_size])
            model.fit(state, modelOutput, epochs= 1, verbose= 0, callbacks= [self.lossHistory])
        
        return model

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description= 'Processes agent parameters.')
    parser.add_argument('-r', '--render', action= 'store_true', help= 'Boolean flag for if the user wants the game environment to render during play')
    parser.add_argument('-l', '--load', action= 'store_true', help= 'Boolean flag for if the user wants to load pre-existing weights')
    parser.add_argument('-e', '--episodes', type= int, default= 10, help= 'Intger representing the number of training rounds to go through, checkpoints are made at the end of each episode')
    args = parser.parse_args()
    qAgent = DeepQAgent(render= args.render, load= args.load)
    qAgent.train(review= True, episodes= args.episodes)
